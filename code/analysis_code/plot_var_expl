#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Estimate how many components are needed to explain 80% of variance at each layer.
Make a plot to visualize this over several example datasets.
"""

import matplotlib.pyplot as plt
import os
import numpy as np

#%% get the data ready to go...then can run any below cells independently.
root = '/usr/local/serenceslab/maggie/biasCNN/';

dataset_all = 'SpatFreqGratings'
#dataset_all = 'SquareGratings'

model='vgg16'

os.chdir(os.path.join(root, 'code', 'analysis_code'))
figfolder = os.path.join(root, 'figures')

import load_activations

param_str='params1'

#%% load activations from whatever networks/training schemes are of interest here

training_strs=['scratch_imagenet_rot_0_square','scratch_imagenet_rot_22_square','scratch_imagenet_rot_45_square'];
ckpt_strs=['350000','350000','350000']
legend_strs=['trained 0 square - 350K steps','trained 22 square - 350K steps','trained 45 square - 350K steps']

nTrainingSchemes = np.size(training_strs)

bigv = []


# load activations for each training scheme
for tr in range(nTrainingSchemes):
  
  training_str = training_strs[tr]
  ckpt_str = ckpt_strs[tr]
  
  # first, searching for all folders from the same model, evaluated on different datasets (the sets are very similar but have different noise instantiation)
  dirs = os.listdir(os.path.join(root, 'activations', model, training_str, param_str))
  good = [ii for ii in range(np.size(dirs)) if dataset_all in dirs[ii]]

  model_name_2plot = model + '_' + training_str + '_' + param_str + '_' + dataset_all + '_avg_samples'
  
  allv = []
  for ii in good:
    
    # also searching for all evaluations at different timepts (nearby, all are around the optimal point)
    dirs2 = os.listdir(os.path.join(root, 'activations', model, training_str, param_str,dirs[ii]))
    nums=[dir[np.char.find(dir,'-')+1:np.char.find(dir,'-')+7] for dir in dirs2]
    
    # compare the first two characters
    good2 = [jj for jj in range(np.size(dirs2)) if 'reduced' in dirs2[jj] and not 'sep_edges' in dirs2[jj] and ckpt_str[0:2] in nums[jj][0:2]]
  
    for jj in good2:
      

      ckpt_num= dirs2[jj].split('_')[2][5:]
      this_allw, all_labs, this_allv, info = load_activations.load_activ(model, dirs[ii], training_str, param_str, ckpt_num)
      allv.append(this_allv)
      if ii==good[0] and jj==good2[0]:
        info_orig = info
      else:
        np.testing.assert_equal(info_orig, info)
        
      # extract some fields that will help us process the data
      orilist = info['orilist']
      phaselist=  info['phaselist']
      sflist = info['sflist']
      typelist = info['typelist']
      noiselist = info['noiselist']
      exlist = info['exlist']
      contrastlist = info['contrastlist']
      
      nLayers = info['nLayers']
      nPhase = info['nPhase']
      nSF = info['nSF']
      nType = info['nType']
      nTimePts = info['nTimePts']
      nNoiseLevels = info['nNoiseLevels']
      nEx = info['nEx']
      nContrastLevels = info['nContrastLevels']
      
      layer_labels = info['layer_labels']
      timepoint_labels = info['timepoint_labels']
      noise_levels = info['noise_levels']    
      stim_types = info['stim_types']
      phase_vals = info['phase_vals']
      contrast_levels = info['contrast_levels']      
      sf_vals = info['sf_vals']
      
      assert nLayers == info['nLayers']
      assert nPhase == info['nPhase']
      assert nSF == info['nSF']
      assert nType == info['nType']
      assert nTimePts == info['nTimePts']
      assert nNoiseLevels == info['nNoiseLevels']
      assert nEx == info['nEx']
      assert nContrastLevels == info['nContrastLevels']
  
  # how many total evaluations of the network do we have here?  
  if tr==0:
    nSamples = np.shape(allv)[0]
  else:
    assert(np.shape(allv)[0]==nSamples)

  bigv.append(allv)

#%%
tr=0
plt.close('all')
layers2plot = np.arange(0,nLayers,1)
 
ncomp_avg = np.zeros([nLayers,1])

for ll in range(nLayers):
  
  cumvar = np.zeros([nTrainingSchemes, nSamples, 500])
  
  for tr in range(nTrainingSchemes):
    
    for kk in range(nSamples):
   
      dat = bigv[tr][kk][ll][0]
      
      plt.subplot(np.ceil(len(layers2plot)/4), 4, ll+1)
        
      plt.plot(np.arange(1,np.size(dat)+1), np.cumsum(dat))
      
      plt.title('%s'%layer_labels[ll])
      
      cumvar[tr,kk,:] = np.cumsum(dat)
      
      plt.ylim([0,1])
      
      plt.axhline(0.80,color='k')
      
      if ll<20:
        plt.xticks([])
      else:
        plt.xlabel('PC number')
        plt.xticks(np.arange(0,501,100))
      
          
  print('\nlayer %s:'%layer_labels[ll])
  
  meancumvar = np.mean(np.mean(cumvar,0),0)
  ncomp = np.where(meancumvar>0.80)[0]
  if np.size(ncomp)>0:

    print('ncompneeded = %d'%ncomp[0])
    ncomp_avg[ll] = ncomp[0]
  else:
    print('>500 comp needed')
    ncomp_avg[ll]=500
plt.suptitle('Cumulative variance explained\n(lines are different training and/or evaluation sets)')
    
  
  
  
